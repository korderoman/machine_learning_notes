{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# REALIZAMOS LAS IMPORTACIONES",
   "id": "78884f3205504cb7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from roboflow import Roboflow\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import os\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "from tqdm import tqdm"
   ],
   "id": "fe89a097aa15811f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# DESCARGAMOS EL DATASET",
   "id": "62f2165f47021d1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "#download: https://github.com/ultralytics/assets/releases/download/v0.0.0/brain-tumor.zip",
   "id": "1d001acfff978030"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# DEFINIMOS EL BLOQUE DE CONVOLUCIÓN",
   "id": "80595134776d2a8a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def conv_block(in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.LeakyReLU(0.1)\n",
    "    )"
   ],
   "id": "a0704d953e2f5b60"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# CREÁMOS NUESTRA RED YOLOv1",
   "id": "c2b72e3929aefe25"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class YOLOv1_BN(nn.Module):\n",
    "    def __init__(self, S=7, B=2, C=1):\n",
    "        #super(YOLOv1_BN, self).__init__()\n",
    "        super().__init__()\n",
    "        self.S = S\n",
    "        self.B = B\n",
    "        self.C = C\n",
    "        self.layers=[self.layer_1(),self.layer_2(),self.layer_3(),self.layer_4(),self.layer_5(),self.layer_6()]\n",
    "        self.features = nn.Sequential(*self.layers)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1024 * self.S * self.S, 4096),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(4096, self.S * self.S * (self.C + self.B * 5))\n",
    "        )\n",
    "\n",
    "    def layer_1(self):\n",
    "        return nn.Sequential( conv_block(3, 64, 7, stride=2, padding=3),nn.MaxPool2d(2, 2),)\n",
    "    def layer_2(self):\n",
    "       return nn.Sequential(conv_block(64, 192, 3, padding=1),\n",
    "        nn.MaxPool2d(2, 2))\n",
    "    def layer_3(self):\n",
    "        return nn.Sequential(conv_block(192, 128, 1),\n",
    "        conv_block(128, 256, 3, padding=1),\n",
    "        conv_block(256, 256, 1),\n",
    "        conv_block(256, 512, 3, padding=1),\n",
    "        nn.MaxPool2d(2, 2))\n",
    "\n",
    "    def layer_4(self):\n",
    "        blocks = [\n",
    "        nn.Sequential(\n",
    "            conv_block(512, 256, 1),\n",
    "            conv_block(256, 512, 3, padding=1)\n",
    "        ) for _ in range(4)\n",
    "        ]\n",
    "        return nn.Sequential(*blocks,  conv_block(512, 512, 1),\n",
    "            conv_block(512, 1024, 3, padding=1),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "    def layer_5(self):\n",
    "        blocks = [\n",
    "        nn.Sequential(\n",
    "            conv_block(1024, 512, 1),\n",
    "            conv_block(512, 1024, 3, padding=1)\n",
    "        ) for _ in range(2)\n",
    "        ]\n",
    "        return nn.Sequential(*blocks,\n",
    "            conv_block(1024, 1024, 3, padding=1),\n",
    "            conv_block(1024, 1024, 3, stride=2, padding=1)\n",
    "        )\n",
    "\n",
    "    def layer_6(self):\n",
    "        return nn.Sequential(\n",
    "            conv_block(1024, 1024, 3, padding=1),\n",
    "            conv_block(1024, 1024, 3, padding=1)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x.view(-1, self.S, self.S, self.C + self.B * 5)\n"
   ],
   "id": "bf44957b76830974"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model = YOLOv1_BN()\n",
    "x = torch.randn(1, 3, 448, 448)\n",
    "out = model(x)\n",
    "print(out.shape)"
   ],
   "id": "aedc599c602955c6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# DATA LOADER",
   "id": "6206a05653f99820"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class TumorDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir, S=7, B=2, C=1):\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.transform = T.Compose([T.Resize((448, 448)), T.ToTensor()])\n",
    "        self.S = S\n",
    "        self.B = B\n",
    "        self.C = C\n",
    "        #al final image_filenames tendrá la lista de todas las imágenes [\"tumor1.jpg\",\"tumor2.jpg\",...]\n",
    "        self.image_filenames = [f for f in os.listdir(image_dir) if f.endswith(\".jpg\") or f.endswith(\".png\")]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_filename = self.image_filenames[idx] # obtenemos la imagen acorde a su indice\n",
    "        image_path = os.path.join(self.image_dir, image_filename) # obtenemos la ruta de la imagen\n",
    "        # obtenemos la ruta del archivo de texto que contiene las etiquetas que debe tener el mismo nombre que la imagen\n",
    "        label_path = os.path.join(self.label_dir, image_filename.replace(\".jpg\", \".txt\").replace(\".png\", \".txt\"))\n",
    "        \"\"\"\n",
    "        Usa PIL.Image para abrir la imagen desde el disco\n",
    "        .convert(\"RGB\") asegura que la imagen tenga 3 canales (aunque venga en escala de grises o RGBA)\n",
    "        \"\"\"\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        \"\"\"\n",
    "            El modelo YOLOv1 devuelve una salida del mismo tamaño: (S, S, C + B*5), donde por cada celda predice:\n",
    "\n",
    "            C scores de clase\n",
    "\n",
    "            B bounding boxes, cada uno con:\n",
    "\n",
    "            x, y: centro (relativo a la celda)\n",
    "\n",
    "            w, h: tamaño (relativo a la imagen)\n",
    "\n",
    "            conf: confianza de que hay un objeto\n",
    "\n",
    "            Para que el loss pueda comparar la salida del modelo con la verdad del dataset, necesitas que tus etiquetas reales (ground truth) estén en ese mismo formato → y ahí entra label_tensor.\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        Para una imagen con una etiqueta, el label_tensor contendrá valores diferentes de 0 solo en una celda. Por ejemplo, si un tumor está en la celda (3, 5):\n",
    "        label_tensor[3, 5] = [clase, conf, x_cell, y_cell, w, h, 0, 0, 0, 0, 0]\n",
    "            clase = 0 si solo tienes una\n",
    "            conf = 1 porque hay un objeto\n",
    "            x_cell, y_cell: posición dentro de la celda (0–1)\n",
    "            w, h: tamaño del bbox\n",
    "            El resto queda en cero (segunda caja no se usa)\n",
    "        Si es que no existe el label tensor no se podría\n",
    "        No podrías:\n",
    "\n",
    "            Comparar correctamente la salida del modelo vs la verdad\n",
    "\n",
    "            Usar loss_fn(preds, label_tensor)\n",
    "\n",
    "            Entrenar correctamente YOLOv1, que espera una estructura específica\n",
    "        \"\"\"\n",
    "        label_tensor = torch.zeros((self.S, self.S, self.C + self.B * 5))\n",
    "\n",
    "        \"\"\"\n",
    "        Comprueba si el archivo .txt que contiene las etiquetas para esa imagen existe. Si no existe, simplemente deja el label_tensor lleno de ceros (es decir, sin objetos).\n",
    "        \"\"\"\n",
    "        if os.path.exists(label_path):\n",
    "            \"\"\"\n",
    "            Abre el archivo de etiquetas y recorre cada línea, que representa un objeto anotado.\n",
    "            \"\"\"\n",
    "            with open(label_path) as f:\n",
    "                for line in f.readlines():\n",
    "                    \"\"\"\n",
    "                    ▶️ Extrae los 5 valores de cada línea:\n",
    "                        class_id: ID de la clase (normalmente 0 si solo tienes una clase, como \"tumor\")\n",
    "                        x, y: coordenadas del centro del objeto, normalizadas (entre 0 y 1)\n",
    "                        w, h: ancho y alto del bounding box, normalizados\n",
    "                    \"\"\"\n",
    "                    class_id, x, y, w, h = map(float, line.strip().split())\n",
    "                    \"\"\"\n",
    "                     Esto ubica el objeto en la celda (j, i) de la cuadrícula S x S (por ejemplo, 7x7):\n",
    "                        x * S: nos da la columna dentro de la cuadrícula\n",
    "                        y * S: nos da la fila\n",
    "                        Ejemplo:\n",
    "                        Si x = 0.73 y S = 7 → int(0.73 * 7) = 5 → columna 5\n",
    "                    \"\"\"\n",
    "                    i = int(x * self.S) # nos da la columna dentro de la cuadrícula\n",
    "                    j = int(y * self.S) # nos da la fila dentro de la cuadrícula\n",
    "                    \"\"\"\n",
    "                    Calcula coordenadas relativas a la celda\n",
    "                    Esto da la posición del centro del objeto dentro de la celda (i, j) como un valor entre 0 y 1.\n",
    "                    Ejemplo:\n",
    "                    Si x * 7 = 5.12, y i = 5 → x_cell = 0.12\n",
    "                    \"\"\"\n",
    "                    x_cell = x * self.S - i\n",
    "                    y_cell = y * self.S - j\n",
    "                    # El ancho y alto ya están normalizados por eso se usan directamente\n",
    "                    w_cell = w # guardamos el ancho\n",
    "                    h_cell = h # guardamos el alto\n",
    "\n",
    "                    # Asignar al primer bbox\n",
    "                    if label_tensor[j, i, self.C] == 0: # Verificamos si el label tenso está vacío\n",
    "                        label_tensor[j, i, self.C] = 1  # confidencia\n",
    "                        label_tensor[j, i, self.C + 1:self.C + 5] = torch.tensor([x_cell, y_cell, w_cell, h_cell])\n",
    "                        label_tensor[j, i, 0] = class_id  # clase (una sola clase = 0)\n",
    "\n",
    "        return image, label_tensor\n"
   ],
   "id": "12c409fd3be641db"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Calculo del IoU",
   "id": "c1ec577149d0b73e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def intersection_over_union(boxes_preds, boxes_labels):\n",
    "    \"\"\"\n",
    "    boxes_preds: [x, y, w, h] formato centrado\n",
    "    boxes_labels: [x, y, w, h] formato centrado\n",
    "    \"\"\"\n",
    "    box1_x1 = boxes_preds[0] - boxes_preds[2] / 2\n",
    "    box1_y1 = boxes_preds[1] - boxes_preds[3] / 2\n",
    "    box1_x2 = boxes_preds[0] + boxes_preds[2] / 2\n",
    "    box1_y2 = boxes_preds[1] + boxes_preds[3] / 2\n",
    "\n",
    "    box2_x1 = boxes_labels[0] - boxes_labels[2] / 2\n",
    "    box2_y1 = boxes_labels[1] - boxes_labels[3] / 2\n",
    "    box2_x2 = boxes_labels[0] + boxes_labels[2] / 2\n",
    "    box2_y2 = boxes_labels[1] + boxes_labels[3] / 2\n",
    "\n",
    "    x1 = max(box1_x1, box2_x1)\n",
    "    y1 = max(box1_y1, box2_y1)\n",
    "    x2 = min(box1_x2, box2_x2)\n",
    "    y2 = min(box1_y2, box2_y2)\n",
    "\n",
    "    intersection = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "\n",
    "    box1_area = (box1_x2 - box1_x1) * (box1_y2 - box1_y1)\n",
    "    box2_area = (box2_x2 - box2_x1) * (box2_y2 - box2_y1)\n",
    "\n",
    "    union = box1_area + box2_area - intersection + 1e-6\n",
    "\n",
    "    return intersection / union"
   ],
   "id": "f78553380a724f69"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# YOLOLOSS",
   "id": "7f56175b05cf36e1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class YOLOLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    S: cuadrícula S×S (ej. 7x7)\n",
    "    B: cantidad de bounding boxes por celda\n",
    "    C: cantidad de clases\n",
    "    λ_coord: penalización más fuerte a errores en coordenadas\n",
    "    λ_noobj: penalización más débil cuando no hay objeto\n",
    "    \"\"\"\n",
    "    def __init__(self, S=7, B=2, C=1, lambda_coord=5, lambda_noobj=0.5):\n",
    "        super(YOLOLoss, self).__init__()\n",
    "        #Define la función de pérdida como Error Cuadrático Medio para todas las partes (coordenadas, clase, confianza)\n",
    "        self.mse = nn.MSELoss(reduction='sum')\n",
    "        self.S = S\n",
    "        self.B = B\n",
    "        self.C = C\n",
    "        self.lambda_coord = lambda_coord\n",
    "        self.lambda_noobj = lambda_noobj\n",
    "\n",
    "    def forward(self, predictions, target):\n",
    "        N = predictions.size(0)\n",
    "        loss = 0\n",
    "        #Itera cada imagen del batch, y cada celda (i, j) en la cuadrícula\n",
    "        for n in range(N):\n",
    "            for i in range(self.S):\n",
    "                for j in range(self.S):\n",
    "                    pred = predictions[n, i, j]\n",
    "                    truth = target[n, i, j]\n",
    "\n",
    "                    has_obj = truth[self.C]\n",
    "                    if has_obj == 1:\n",
    "                        # Comparar los dos bounding boxes\n",
    "                        ious = []\n",
    "                        for b in range(self.B):\n",
    "                            start = self.C + b * 5 + 1\n",
    "                            box_pred = pred[start:start+4]\n",
    "                            box_true = truth[self.C+1:self.C+5]\n",
    "                            iou = intersection_over_union(box_pred, box_true)\n",
    "                            ious.append(iou)\n",
    "                        best_box = torch.argmax(torch.tensor(ious))\n",
    "\n",
    "                        # Coordenadas y confianza del mejor bbox\n",
    "                        start = self.C + best_box * 5\n",
    "                        pred_box = pred[start+1:start+5]\n",
    "                        true_box = truth[self.C+1:self.C+5]\n",
    "\n",
    "                        pred_conf = pred[start]\n",
    "                        true_conf = truth[self.C]\n",
    "\n",
    "                        # Pérdida de clase (opcional)\n",
    "                        loss += self.mse(pred[0:self.C], truth[0:self.C])\n",
    "\n",
    "                        # Pérdida de bbox (solo del mejor)\n",
    "                        loss += self.lambda_coord * self.mse(pred_box, true_box)\n",
    "\n",
    "                        # Pérdida de confianza del mejor bbox\n",
    "                        loss += self.mse(pred_conf, true_conf)\n",
    "\n",
    "                        # Penalizar confianza de los otros bboxes\n",
    "                        for b in range(self.B):\n",
    "                            if b != best_box:\n",
    "                                conf = pred[self.C + b * 5]\n",
    "                                loss += self.lambda_noobj * self.mse(conf, torch.tensor(0.))\n",
    "                    else:\n",
    "                        # No hay objeto → penalizar ambos bboxes\n",
    "                        for b in range(self.B):\n",
    "                            conf = pred[self.C + b * 5]\n",
    "                            loss += self.lambda_noobj * self.mse(conf, torch.tensor(0.))\n",
    "\n",
    "        return loss / N\n"
   ],
   "id": "73c6e2dffcaeff2e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ENTRENAMIENTO",
   "id": "3fe1e50ec77f5e61"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "def train_yolov1(\n",
    "    model,\n",
    "    dataset,\n",
    "    loss_fn,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    epochs=50,\n",
    "    batch_size=16,\n",
    "    lr=1e-4,\n",
    "    checkpoint_path=\"yolov1_final.pth\",\n",
    "    save_best=True\n",
    "):\n",
    "    # DataLoader\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "    # Preparación del modelo\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.5)\n",
    "\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "\n",
    "        loop = tqdm(dataloader, leave=False)\n",
    "        for imgs, labels in loop:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "            preds = model(imgs)\n",
    "            loss = loss_fn(preds, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            loop.set_description(f\"Epoch [{epoch+1}/{epochs}]\")\n",
    "            loop.set_postfix(loss=loss.item(), avg_loss=epoch_loss / (loop.n + 1))\n",
    "\n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"📉 Epoch {epoch+1}/{epochs} — Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # Guardar mejor modelo\n",
    "        if save_best and avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "            print(f\"✅ Nuevo mejor modelo guardado (Loss: {best_loss:.4f})\")\n",
    "\n",
    "        # Checkpoints periódicos\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            ckpt_name = f\"checkpoint_epoch{epoch+1}.pth\"\n",
    "            torch.save(model.state_dict(), ckpt_name)\n",
    "            print(f\"💾 Checkpoint guardado: {ckpt_name}\")\n",
    "\n",
    "    # Guardar modelo final\n",
    "    torch.save(model.state_dict(), checkpoint_path)\n",
    "    print(f\"🎯 Modelo final guardado en: {checkpoint_path}\")\n",
    "\n"
   ],
   "id": "70f1bdad5fb4c387"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model = YOLOv1_BN(S=7, B=2, C=1)\n",
    "loss_fn = YOLOLoss(S=7, B=2, C=1)\n",
    "\n",
    "train_dataset = TumorDataset(\n",
    "    image_dir = r\"D:\\programacion\\proyectos\\utec\\IA\\modulo_2_redes_convolucionales_y_generativas\\yolo\\brain-tumor\\train\\images\",\n",
    "    label_dir = r\"D:\\programacion\\proyectos\\utec\\IA\\modulo_2_redes_convolucionales_y_generativas\\yolo\\brain-tumor\\train\\labels\",\n",
    "    S=7, B=2, C=1,\n",
    ")\n",
    "\n",
    "\n",
    "train_yolov1(\n",
    "    model=model,\n",
    "    dataset=train_dataset,\n",
    "    loss_fn=loss_fn,\n",
    "    epochs=50,\n",
    "    batch_size=16,\n",
    "    lr=1e-4\n",
    ")"
   ],
   "id": "b7f130825f8426cf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Evaluación del modelo",
   "id": "8074bf8bb7704b60"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model = YOLOv1_BN(S=7, B=2, C=1)\n",
    "model.load_state_dict(torch.load(\"yolov1_final.pth\", map_location=torch.device(\"cpu\")))\n",
    "model.eval()"
   ],
   "id": "36b946bb94ef7aa5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Creamos una función de predicción",
   "id": "338423cc5d5bd5c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torchvision.transforms as T\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_prediction_on_image(model, image_path, S=7, B=2, C=1, conf_threshold=0.4):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    transform = T.Compose([T.Resize((448, 448)), T.ToTensor()])\n",
    "\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    original_w, original_h = image.size\n",
    "    input_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor).squeeze(0).cpu()\n",
    "\n",
    "    best_conf = 0\n",
    "    best_box = None\n",
    "    for i in range(S):\n",
    "        for j in range(S):\n",
    "            cell = output[i, j]\n",
    "            for b in range(B):\n",
    "                conf = cell[C + b * 5]\n",
    "                if conf > best_conf and conf > conf_threshold:\n",
    "                    x_rel, y_rel, w_rel, h_rel = cell[C + b * 5 + 1:C + b * 5 + 5]\n",
    "                    best_conf = conf\n",
    "                    best_box = (j, i, x_rel, y_rel, w_rel, h_rel)\n",
    "\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    if best_box:\n",
    "        j, i, x_cell, y_cell, w, h = best_box\n",
    "        x_center = (j + x_cell) / S * original_w\n",
    "        y_center = (i + y_cell) / S * original_h\n",
    "        width = w * original_w\n",
    "        height = h * original_h\n",
    "\n",
    "        x1 = x_center - width / 2\n",
    "        y1 = y_center - height / 2\n",
    "        x2 = x_center + width / 2\n",
    "        y2 = y_center + height / 2\n",
    "\n",
    "        draw.rectangle([x1, y1, x2, y2], outline=\"red\", width=3)\n",
    "        draw.text((x1, y1 - 10), f\"conf: {best_conf:.2f}\", fill=\"red\")\n",
    "    else:\n",
    "        print(\"⚠️ No se detectó ningún objeto con suficiente confianza.\")\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(image)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Predicción YOLOv1 - Tumor cerebral\")\n",
    "    plt.show()\n"
   ],
   "id": "e5cf2bee915873e0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Implementamos",
   "id": "19c62443ab509fc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "show_prediction_on_image(model, r\"D:\\programacion\\proyectos\\utec\\IA\\modulo_2_redes_convolucionales_y_generativas\\yolo\\brain-tumor\\valid\\images\\val_1 (1).jpg\")\n",
   "id": "b9308309a47b31c2"
  }
 ],
 "metadata": {},
 "nbformat": 5,
 "nbformat_minor": 9
}
