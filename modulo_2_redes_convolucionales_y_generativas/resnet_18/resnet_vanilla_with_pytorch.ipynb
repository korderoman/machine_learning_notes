{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T00:51:45.455276Z",
     "start_time": "2025-03-20T00:51:44.330984Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n"
   ],
   "id": "c7cb7196da4d21a2",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# Definir el bloque residual",
   "id": "854e193b3ddf0b7d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T00:51:52.315918Z",
     "start_time": "2025-03-20T00:51:52.311980Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    La variable expansion se usa en la arquitectura de ResNet para definir cu√°ntos canales de salida tendr√° el bloque residual en relaci√≥n con su entrada.\n",
    "    Esto significa que la cantidad de canales de salida es la misma que la cantidad de canales internos en el bloque.\n",
    "\n",
    "    Ejemplo en ResNet-18:\n",
    "\n",
    "    Si entran 64 canales, la salida del bloque tambi√©n tiene 64 canales.\n",
    "    Si entran 128 canales, la salida tambi√©n tiene 128 canales.\n",
    "    No hay aumento en los canales.\n",
    "    \"\"\"\n",
    "    expansion = 1  # Para ResNet-18 y ResNet-34\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        \"\"\"\n",
    "        nn.Conv2d: Crea una capa convolucional 2D; in_channels, representa el n√∫mero de canales de entrada, por ejemplo una imagen RGB tiene 3 canales; out_channels es el n√∫mero de filtros que se aplicaran tal que cada uno genera una salida; kernel_size=3 , es el tama√±o del filtro; stride indica el desplazamiento del filtro en cada paseo; por ejemplo si el stride es de 1 significa que el filtro se mueve 1 pixel a la vez. Un stride 2 reduce a la mitad el tama√±o espacial, el padding=1 agrega 1 pixel de borde (cero) alrededor de la imagen de entrada para conservar su tama√±o original al aplicar una convoluci√≥n y finalmente el bias=False, indicamos que no existir√° sesgo; esto es muy com√∫n cuando se usa BatchNorm luego de la convoluci√≥n.\n",
    "        \"\"\"\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        \"\"\"\n",
    "        nn.BatchNorm2d: Normaliza la salida de una capa convolucional para cada mini-lote, canal por canal; es decir, toma la salida de la convoluci√≥n y le aplica una transformaci√≥n para estabilizar el entrenamiento\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        ‚úÖ Prop√≥sito principal\n",
    "            Estabiliza el entrenamiento al mantener activaciones con media cercana a 0 y varianza cercana a 1.\n",
    "            Acelera la convergencia (entrena m√°s r√°pido).\n",
    "            Reduce el problema de covariate shift interno, es decir, los cambios en la distribuci√≥n de activaciones dentro del modelo.\n",
    "            Permite usar tasas de aprendizaje m√°s altas sin que el entrenamiento explote.\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "         ¬øPor qu√© out_channels?\n",
    "            Porque la normalizaci√≥n se aplica por canal, y la salida de la convoluci√≥n anterior tiene out_channels mapas de activaci√≥n (uno por filtro). Entonces:\n",
    "        \"\"\"\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \"\"\"\n",
    "        Si el canal de entrada y el canal de salidad es el mismo o el stride sigue siendo 1 el shortcut es solo  una identidad;  caso contrario se aplica una convolucional 1x1 con el stride recibido como argumento para que la forma coincida con la salida\n",
    "        \"\"\"\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        ¬øPor qu√© revisamos stride != 1?\n",
    "        El stride en la primera convoluci√≥n del bloque puede ser diferente de 1 (normalmente 2), lo que significa que la imagen se reduce de tama√±o (submuestreo).\n",
    "\n",
    "        üëâ Ejemplo:\n",
    "        Si la entrada tiene tama√±o [batch, 64, 32, 32] y usas:\n",
    "\n",
    "        nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n",
    "        La salida ser√° de tama√±o [batch, 128, 16, 16]\n",
    "        ‚õî Pero tu identity = x sigue teniendo tama√±o [batch, 64, 32, 32]\n",
    "        ‚õî ¬°No puedes sumarlos directamente!\n",
    "\n",
    "        Entonces, en este caso, necesitas ajustar la shortcut para que tambi√©n reduzca la resoluci√≥n con stride=2.\n",
    "        ¬øPor qu√© revisamos in_channels != out_channels?\n",
    "        Incluso si el tama√±o espacial es el mismo (stride=1), puede cambiar el n√∫mero de canales.\n",
    "        Por ejemplo:\n",
    "\n",
    "        nn.Conv2d(64, 128, kernel_size=3, stride=1)\n",
    "        Entonces:\n",
    "\n",
    "        Salida del bloque ‚Üí [batch, 128, H, W]\n",
    "        Entrada (x) ‚Üí [batch, 64, H, W] ‚õî Nuevamente, no puedes sumarlos.\n",
    "        üìå Necesitas una convoluci√≥n 1√ó1 que cambie los canales de la entrada (x) a 128.\n",
    "        :param x:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        identity = self.shortcut(x)\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = nn.ReLU()(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out += identity  # Suma residual\n",
    "        out = nn.ReLU()(out)\n",
    "        return out\n"
   ],
   "id": "849ccbcea5528cbf",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Construir la red ResNet-18",
   "id": "e988c0fb90f79a4a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T00:51:56.054263Z",
     "start_time": "2025-03-20T00:51:56.049619Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ResNet18(nn.Module):\n",
    "    def __init__(self, num_classes=10):  # CIFAR-10 tiene 10 clases\n",
    "        super(ResNet18, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.layer1 = self._make_layer(64, 64, 2, stride=1)\n",
    "        self.layer2 = self._make_layer(64, 128, 2, stride=2)\n",
    "        self.layer3 = self._make_layer(128, 256, 2, stride=2)\n",
    "        self.layer4 = self._make_layer(256, 512, 2, stride=2)\n",
    "        \"\"\"\n",
    "        ‚úÖ Aplica pooling global adaptativo para convertir cualquier mapa de activaci√≥n de tama√±o arbitrario a un tama√±o fijo de 1√ó1 por canal.\n",
    "        En concreto:\n",
    "\n",
    "        Si la entrada tiene forma [batch_size, canales, alto, ancho]\n",
    "        La salida ser√°: [batch_size, canales, 1, 1]\n",
    "        Es decir:\n",
    "\n",
    "        Promedia todos los valores espaciales (alto √ó ancho) dentro de cada canal\n",
    "        Pero lo hace de manera autom√°tica, sin que tengas que especificar el tama√±o exacto de la entrada\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        üß© ¬øPor qu√© \"adaptativo\"?\n",
    "            Porque funciona con cualquier tama√±o de entrada.\n",
    "\n",
    "            No necesitas saber si la entrada ser√° 4√ó4, 8√ó8 o 7√ó7\n",
    "            Siempre la va a reducir a 1√ó1\n",
    "            Muy √∫til cuando la arquitectura puede variar o la entrada no es de tama√±o fijo\n",
    "        \"\"\"\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def _make_layer(self, in_channels, out_channels, blocks, stride):\n",
    "        layers = []\n",
    "        layers.append(BasicBlock(in_channels, out_channels, stride))\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(BasicBlock(out_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        \"\"\"\n",
    "        La funci√≥n torch.flatten(input, start_dim) aplana (convierte en vector) las dimensiones a partir de start_dim hasta el final.\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        Ejemplo sea x.shape = [128, 512, 1, 1]\n",
    "        Necesitamos convertirlo en [128, 512] para poder pasarlo a una nn.Linear (que espera un vector plano).\n",
    "        x = torch.flatten(x, 1)\n",
    "        produce x.shape = [128, 512]\n",
    "        \"\"\"\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Crear un modelo ResNet-18 desde cero\n",
    "#model = ResNet18()\n",
    "#print(model)\n"
   ],
   "id": "bdc7808cecea3cf7",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "93178e8d4ffea966"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ------------------------------------------------ FIN de creaci√≥n de la red ResNet-18 ------------------------------------------------",
   "id": "abcba75e9785da63"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Carga de datos CIFAR-10",
   "id": "e5d10c3050f4271f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T00:52:01.259097Z",
     "start_time": "2025-03-20T00:51:59.624839Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Configurar GPU si est√° disponible\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando dispositivo: {device}\")\n",
    "\n",
    "# Transformaciones para normalizar los datos y aplicar data augmentation\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),  # Aumentaci√≥n de datos\n",
    "    transforms.RandomHorizontalFlip(),  # Flip aleatorio\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))  # Normalizaci√≥n\n",
    "])\n",
    "\"\"\"\n",
    "Los n√∫meros que se usan en la normalizaci√≥n de CIFAR-10 tienen un prop√≥sito espec√≠fico y no son elegidos al azar. Vamos a explicarlo bien.\n",
    "transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
    "Esta l√≠nea normaliza cada canal (R, G, B) de las im√°genes, de la siguiente forma: pixel_normalized = (pixel - mean) / std\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
    "])\n",
    "\n",
    "# Cargar dataset CIFAR-10\n",
    "batch_size = 128\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "# Definir clases de CIFAR-10\n",
    "classes = ('avi√≥n', 'autom√≥vil', 'p√°jaro', 'gato', 'ciervo', 'perro', 'rana', 'caballo', 'barco', 'cami√≥n')"
   ],
   "id": "8938353910bb2ab9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cpu\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Cargar el modelo ResNet-18",
   "id": "d2800f2622b5b7af"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T00:52:04.273832Z",
     "start_time": "2025-03-20T00:52:04.220179Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Cargar el modelo en el dispositivo\n",
    "model = ResNet18(num_classes=10).to(device)\n",
    "\n",
    "# Definir funci√≥n de p√©rdida y optimizador\n",
    "criterion = nn.CrossEntropyLoss()  # P√©rdida para clasificaci√≥n\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)  # Adam con regularizaci√≥n L2\n",
    "\n",
    "# Usar un Scheduler para reducir la tasa de aprendizaje con el tiempo\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)"
   ],
   "id": "351e146f3fe3c523",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Entrenamiento del modelo",
   "id": "57e12f5a7e18a2b4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T13:17:34.472666Z",
     "start_time": "2025-03-20T02:03:43.099920Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_epochs = 20  # N√∫mero de √©pocas\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Modo entrenamiento\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in trainloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()  # Resetear gradientes\n",
    "        outputs = model(images)  # Forward pass\n",
    "        loss = criterion(outputs, labels)  # Calcular p√©rdida\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Actualizar pesos\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    scheduler.step()  # Actualizar learning rate\n",
    "\n",
    "    epoch_loss = running_loss / len(trainloader)\n",
    "    epoch_acc = 100 * correct / total\n",
    "    print(f\"√âpoca [{epoch+1}/{num_epochs}], P√©rdida: {epoch_loss:.4f}, Precisi√≥n: {epoch_acc:.2f}%\")\n"
   ],
   "id": "4c0a1bbf6b5221a4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "√âpoca [1/20], P√©rdida: 0.5216, Precisi√≥n: 82.16%\n",
      "√âpoca [2/20], P√©rdida: 0.4819, Precisi√≥n: 83.57%\n",
      "√âpoca [3/20], P√©rdida: 0.4570, Precisi√≥n: 84.37%\n",
      "√âpoca [4/20], P√©rdida: 0.4373, Precisi√≥n: 85.04%\n",
      "√âpoca [5/20], P√©rdida: 0.2948, Precisi√≥n: 90.17%\n",
      "√âpoca [6/20], P√©rdida: 0.2518, Precisi√≥n: 91.47%\n",
      "√âpoca [7/20], P√©rdida: 0.2320, Precisi√≥n: 92.16%\n",
      "√âpoca [8/20], P√©rdida: 0.2131, Precisi√≥n: 92.73%\n",
      "√âpoca [9/20], P√©rdida: 0.1995, Precisi√≥n: 93.19%\n",
      "√âpoca [10/20], P√©rdida: 0.1884, Precisi√≥n: 93.60%\n",
      "√âpoca [11/20], P√©rdida: 0.1787, Precisi√≥n: 93.93%\n",
      "√âpoca [12/20], P√©rdida: 0.1638, Precisi√≥n: 94.40%\n",
      "√âpoca [13/20], P√©rdida: 0.1539, Precisi√≥n: 94.72%\n",
      "√âpoca [14/20], P√©rdida: 0.1491, Precisi√≥n: 95.01%\n",
      "√âpoca [15/20], P√©rdida: 0.1205, Precisi√≥n: 96.02%\n",
      "√âpoca [16/20], P√©rdida: 0.1139, Precisi√≥n: 96.33%\n",
      "√âpoca [17/20], P√©rdida: 0.1114, Precisi√≥n: 96.29%\n",
      "√âpoca [18/20], P√©rdida: 0.1049, Precisi√≥n: 96.53%\n",
      "√âpoca [19/20], P√©rdida: 0.1033, Precisi√≥n: 96.61%\n",
      "√âpoca [20/20], P√©rdida: 0.1012, Precisi√≥n: 96.66%\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Evaluaci√≥n del modelo en datos de prueba",
   "id": "67fc59fd6ba4c2b2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T13:22:20.254261Z",
     "start_time": "2025-03-20T13:19:31.795845Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.eval()  # Modo evaluaci√≥n\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():  # Desactivar gradientes para evaluaci√≥n\n",
    "    for images, labels in testloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Precisi√≥n en el conjunto de prueba: {accuracy:.2f}%\")\n"
   ],
   "id": "a0c0423c229856ed",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisi√≥n en el conjunto de prueba: 91.59%\n"
     ]
    }
   ],
   "execution_count": 18
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
